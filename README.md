# Computer Pointer Controller

The purpose of this Intel/Udacity project is to solidify the knowledge generated by participating in this course, to create an interface or aggregate model 
where either a predetermined video, locally sourced webcam or series of images of a human face (though technically it could be used with other similarly shaped non-human faces) to controll the computer mouse pointer with both the direction of the eye gaze and the pitch of the head position.  

To do so requires the coordination/integration of 4 pre-trained and optimized (xml & bin format) Intel models drawn from the Intel OpenVino Open Model Zoo.

The the purpose of the project is three fold correctly generate all the required python scripts to generate the automated mouse moving behaviour as a minimum, but then to compare the use of different model precisions and different hardware configurations on the efficiency, accuracy and throughput of the model(s) and the impact this has on the overall project deployment.
 
Finally, to see if or what edge cases can be identified using and or tweaking the project inputs and processes (pre and post for each model) using the object detection API, OpenVino Workbench and VTune Profile.

## Project Set Up and Installation

1. Download the OpenVino ToolKit relative to your core operating system (https://software.intel.com/content/www/us/en/develop/tools/openvino-toolkit/choose-download.html) and install it on your machine locally (this project used the 2020.4.287 Linux build of the OpenVino toolkit, but did so on a Windows 10 Pro OS utilising VMWare16 runing Unbuntu 20.04). 

2. source /opt/intel/openvino/bin/setupvars.sh -pyver 3.6.*

3. Enusre the installation is correctly installed and required dependencies included in the build by following the below:

-- 3.1 To generate this version of the mouse pointer project (developed in the Windows 10 Pro environment, native, not wsl 1 or 2) you must clone the repository from this URL: https://github.com/Cowboycommit/JashaBCPC.git

This repository contains all the required shell/bash and python scripts (Py ver 3.6.12 - its advised to use a slightly older version of python that matches the OpenVino requirements but be guided by the OpenVino website). 
The repo also contains all dependencies required to run this project end to end - but as requested in the project brief not the required models, as these will be downloaded as part of the set-up process.

-- 3.2 As per the Rubric requirements you must create and activate a virtual environment in your given working directory.

	cd JashaBCPC (changing to the root directory for the project drawn from the repo)
	python3 -m venv C:/<PATH_to_Environment>/JashaBCPC

 	then activate
	
	source /JashaBCPC/venv/bin/
	activate

-- 3.3 Load the OpenVino Variables from installed directory of OpenVino

        source {INSTALLED_OPENVINO_DIR}/bin/setupvars.sh 

-- 3.4 To ensure the correct dependencies are installed (if you aren't sure) please run the below - optional

	C:/<PATH_to_Environment>/JashaBCPC$ pip install -r requirements.txt


#### Step only required if models arent present on the host environment

4. Models used (not required for submission)

If you don't already have the models please follow the below with the addition of the -o operator (ouput directory) after the model name and add the directions to the JashaBCPC/src folder.

There are 4 core models required from the Intel Open Model Zoo models which are listed below and can be obtained using the OpenVINO model downloader:

**4.1. Face Detection Model**
```
python3 opt/intel/openvino/deployment_tools/tools/model_downloader/downloader.py --name "face-detection-adas-0001"
```
**4.2. Facial Landmarks Detection Model**
```
python3 opt/intel/openvino/deployment_tools/tools/model_downloader/downloader.py --name "landmarks-regression-retail-0009"
```
**4.3. Head Pose Estimation Model**
```
python3 opt/intel/openvino/deployment_tools/tools/model_downloader/downloader.py --name "head-pose-estimation-adas-0001"
```
**4.4 Gaze Estimation Model**
```
python3 opt/intel/openvino/deployment_tools/tools/model_downloader/downloader.py --name "gaze-estimation-adas-0002"

## Demo
The demo of this application is easy if you have followed the correct steps above and have all the require files located in the righ folder structure. 

The main.py script controls the feeding of pre- and post- processed data through the different models initially and they daisey-chain after this.

Below is the demo command line argument to see the demo in action. All model precisions are initially set to FP32 as if the entire app is being run off the CPU only. These can of course be altered as the main directory file has a number of executable .sh scripts which run through the different precision levels INT8-FP32 and of course this can be altered depending on the virtualised or real hardware avaliability, where both INT8 or FP16 can be used (if avaliable with each model used by the app)- a note here is that INT8 models will not run on Rasberry Pi there is a software hardware conflict which is a current an known issue - FP16 & FP32 are (a little slow) but fine.

Below is the actual bash script to run from the main directory, as said previously FPs are all set to 32 for this initial run.  



Reminder, for ease of execution you need only to run the "party_starter32.sh" file in the app root along with the other precisions are already to execute (or just copy and paste the above if you wish mix and match).

## Documentation

Command Line Arguments and Requirements as noted above 

CLI  |Type               |Description
Args |-------------------| ---------------------------------------------------|
-fd  | Mandatory	 | Path to .xml file of Face Detection model.
-fl  | Mandatory	 | Path to .xml file of Facial Landmark Detection model.
-hp  | Mandatory	 | Path to .xml file of Head Pose Estimation model.
-ge  | Mandatory	 | Path to .xml file of Gaze Estimation model.
-i   | Mandatory	 | Path to video file or enter cam for webcam
-d   | Optional	  	 | Provide the target device: CPU (default) / GPU / MYRIAD (ie NCS2) / FPGA
-h   | Optional   	 | Help/descriptor for CLI arguments

The optional features as per the requirements of the project, allow for visual debuging of each output from each model which you normaly would not see. This is usefull as there are clear issues with model control dominance over the mouse cursor in the final output, which will be discussed in the Edge Case section.

Gives the computalionally significat ability to identify cross-platform linker libraries. Also impacts app portability, particularly as this is a python solution which is OS agnostic in terms of read in, but different in internal hierachy.

Target device is not so mucha feature as a requirement - it is important to be able to identify which possible target devices are configured and avaliavble on the given system. i.e. identifies conflics when you have an IGPU and a GPU. The default is obviously CPU no need to indicate with -d

Help file as required for the project. 

[Directory-Structure](bin/Directory_Structure.png)

<h2>JashaBCPC</h2>
- <b>README.md</b> This file 
- <b>.gitignore</b> listing of files that should not be uploaded to GitHub 
- <b>requirements.txt</b> Key project dependencies in this file (will be run as part of setup if required)
- <b>bin</b> folder contains the media files

        
-<b>src</b> folder contains python files of the app
    + [face_detection.py](src/face_detection.py) : Face Detection related inference code
    + [facial_landmarks_detection.py](src/facial_landmarks_detection.py) : Landmark Detection related inference code
    + [gaze_estimation.py](src/gaze_estimation.py) : Gaze Estimation related inference code
    + [head_pose_estimation.py](src/head_pose_estimation.py) : Head Pose Estimation related inference code
    + [input_feeder.py](src/input_feeder.py) : input selection related code
    + [main.py](src/main.py): Coordinating python file
    + [mouse_controller.py](/src/mouse_controller.py) : Mouse Control related utilities.

    
- <b>party_starter8.sh</b> one shot execution script that covers all the prerequisites of the project - INT8
- <b>party_starter16.sh</b> one shot execution script that covers all the prerequisites of the project - FP16
- <b>party_starter32.sh</b> one shot execution script that covers all the prerequisites of the project - FP32

After the following models should be downloaded by the model downloader <INSTAL_DIR><Path_to_model_downloader> i.e. python3 opt/intel/openvino/deployment_tools/tools/model_downloader/downloader.py --name "face-detection-adas-0001" but with the -o functions call and direct them straight to <JashaBCPC_root>/src the downloader will do the rest.They will be in the newly created directory <JashaBCPC_root>/src/models/intel/

- <b>models</b> folder contains pre-trained models from Open Model Zoo
    - intel
        1. face-detection-adas-0001
        2. gaze-estimation-adas-0002
        3. head-pose-estimation-adas-0001
        4. landmarks-regression-retail-0009


## Benchmarks
I conducted a thorough investigation of the App and the models that the App comprises of too understand where there may be bottle necks, how the models perform on different hardware, how the app performs under differenc conditions. The full results can be found in the results folder. 

I did the app and models together and separately using the DL Workbench through the docker image (being this is built in unbuntu and there is no docker desktop) I used the Benchmark_App through the Intel DevCloud and finaly I scripted a lineprofiler into the head/main.py file so that everytime the app is run a report on time and throughput is produced.

Looking at the models separately is important as they are very different in size and also there is a clear conflict between headpose and gaze estimation, which I will discuss in the Edge Cases section.  


## Benchmarks
Benchmark results of the model.

1. Benchmak App Results (multiple devices at all precisions)
	
Single Hardware used from the DevCloud: 
Core i5 6500te, 
Intel® Xeon® Processor E3-1268L v5, 
Intel-hd-530 IGPU, Intel Neural Compute Stick 2 (NCS), 
Intel mustang-v100-mx8 FPG/HDDL-R

MULTI Hardware configs used (DevCloud)

MULTI:CPU Core i5 6500te, GPU hd-530 
MULTI:CPU i5 6500te, HDDL Intel mustang-v100-mx8

s1 Model Results 	
	Facial Recognition
 	Gaze Estimation
	Head pose estimation
	Landmark Regression

s2. Workbench Resusts
	FL16FP.png
	FL16INT8FP.png
	FL32FP.png
	GE16FP.png
	GE16INT8FP.png
	GE32FP.png
	HPE16FP.png
	HPE16INT8FP.png
	LRL16FP.png
	LRR16INT8FP.png
	LRR32FP.png
	WorkBenchw_All_Models_and_Precisions.png

3. App Line profiles outputs
	profile_line.txt
	profile_line8.txt
	profile_line16.txt
	profile_line32.txt

4. Video demo of output
	CPC_supplied_demo1.mp4

## Results
Looked at individual results compared their performances by inference time, frame per second and model loading time, as well as app results by execution time, overhead time and bottle necking. 

As we can see from above graph that FPGA took more time for inference than other device because it programs each gate of fpga for compatible for this application. It can take time but there are advantages of FPGA such as:-
- It is robust meaning it is programmable per requirements unlike other hardware.
- It has also longer life-span.

GPU processed more frames per second compared to any other hardware and specially when model precision is FP16 because GPU has several execution units and their instruction sets are optimized for 16 floating point data types. The GPU at FP16 with all models seems to be the moste efficient and accurate. All the FPG models had far too high overhead or prelaod for this task, on the flip-side in coordination (MULTI:) with the low latency CPU it does process frames at the fastest rate, just slightly higher that and the GPU. 

- I ran the comprising models with different precision, and although precision affects accuracy, on my Intel i71000k with the 650IGPU with 46G of ram. All the models ran very efficiency. The as can be seen in the workbench results the low size models like the gaze estimation, there is almost no throughput difference when moving from FP32-INT8 so these can be set as low as possible which decreases load time and inference requests as they are dramatically lower with not much loss in throughput and virtually no change in latency. Where as the larger models such as the landmarks regression model, throughput drops significantly between FP32 and FP16 (it almost halves) so should be kept higher. So model size can be reduced incremental by lowing the precision from FP32 to FP16 or INT8 to mix and match the App build (depending on the hardware is being targeted) thus, inference becomes faster but because of lowing the precision without good information from a sandbox or test-bed the App/model can drop information (or frames in this case) that lower the overall accuracy of App/model. 

- Although it's true that when you use lower precision model(s) then you can get lower accuracy, in this application that isn't so noticeable as firstly it's running async so will stream data continuously as it's using multi-threading, but also pyautogui actually runs in a given direction from the start and doesn't stop until given its next instruction. This mean's that once the first gaze/headpose coords are given the cursor will run until the next are given. So the output is not noticeably different as seen in the video(s) in results than with a higher precision model, also the data transfer of INT8 and FP16 models is faster given the number of floating points halves each time. What is noticible is the drop in video quality or more specifically frames are dropped, quite obviously, as precision goes down, so pyautogui does suffer some loss. 

- A quick note here is that it is possible to run this on Raspberry pi 4b but the installation is outside of the rubric here, but there is a conflict between OpenVino processing of INT8 model. It is a conflict between the chip architecture and the opencv/openvino processing of the video at the INT8 precision level.  

## Stand Out Suggestions
The biggest standout that I couldn't master was to be able to center or fix the position of the output video and simultaneously have pyautogui center the mouse cursor. Many runs of the model the video randomly appears and if the mouse is too close to the screen edge it falls over.

### Async Inference

### Edge Cases
1. Moving Mouse Pointer out of the maximum window width - Pyautogui has a failsafe which kills the App if the mouse moves beyond the screen dimensions. This is presumably so that people hacking a system can't override manual mouse control via a backdoor. In this instance it's possible to turn this feature off, though not advised and the problem is fixed.
2. The facial detection model can sometimes struggle to identify a face (poor image quality i.e. low probability) or which face to focus on particularly if the feed is poor or confusing to the model. Resolution use quality instream be it cam or video and clearly remove more than one individual from the feed (or retrain the modelwhich is way out of scope) 
4. Wrong Video/Image file given - currently only certain types of images or video files are supported i.e. jpg is not only bmp and png same with the many video formats if the codec is even avaliable often an error can be thrown when the video image resize becomes computationally heavy, essential it breaks the app. 
5. There is a natural conflict between the head pose estimation model and the gaze estimation model. The if someone turns their head one way but looks in the opposite direction as happens in the demo video, then one model will have to override the other. There is no indication as to which way this should happen and it would be case dependent (i.e. what does the client need more eye or head direction) without knowing that there isn't a resolution, but one could be created fairly quickly, even in the execution of each model.   
