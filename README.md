# Computer Pointer Controller

The purpose of this Intel/Udacity project is to solidify the knowledge generated by participating in this course, to create an interface or aggregate model 
where either a predetermined video, locally sourced webcam or series of images of a human face (though technically it could be used with other similarly shaped non-human faces)
to controll the computer mouse pointer with both the direction of the eye gaze and the pitch of the head position.  
To do so requires the coordination/integration of 4 pre-trained and optimized (xml & bin format) Intel models drawn from the Intel OpenVino Open Model Zoo.

The the purpose of the project is three fold correctly generate all the required python scripts to generate the automated mouse moving behaviour as a minimum, but then to 
compare the use of different model precisions and different hardware configurations on the efficiency, accuracy and throughput of the model(s) and the impact this has on the overall project deployment. 
Finally, to see if or what edge cases can be identified using and or tweaking the project inputs and processes (pre and post for each model) using the object detection API, OpenVino WorkBench and VTune Profiler.

## Project Set Up and Installation

1. Download the OpenVino ToolKit relative to your core operating system (https://software.intel.com/content/www/us/en/develop/tools/openvino-toolkit/choose-download.html) and 
install it on your machine locally (this project used the 2020.4.287 Linux build of the OpenVino toolkit, but did so on a Windows 10 Pro OS utilising the ver 2 Linux Subsystem i.e. wsl 2 with Unbuntu 20.04). 
Useful Tip: using the recently released Windows Terminal developers can switch between the same application and directory structure (windows) depending on their OS preference and avaliabity - Linux has many more command line arguments, Windows are more simplified. 

2. source /opt/intel/openvino/bin/setupvars.sh -pyver 3.6

3 Enusre the installation is correctly installed and required dependencies included in the build by following the below:



2. To generate this version of the mouse pointer project (developed in the Windows 10 Pro environment, native, not wsl 1 or 2) you must clone the repository from this URL:
This repository contains all the required shell/bash and python scripts (Py ver 3.6.12 - its advised to use a slightly older version of python that matches the OV requirements but be guided by the OV website) 
The repo also contains all dependencies required to run this project end to end - but as requested in the project brief not the required models, as these will be downloaded as part of the set-up process.

3. As per the Rubric requirements you must create and activate a virtual environment in your given working directory - To achieve this in Windows 10 Pro follow the below example in CMD:

	cd CPC_JB (changing to the headder directory for the project drawn from the repo)
	python3 -m venv C:/<PATH_to_Environment>/CPC_JB

    ##then activate
	
	C:/<PATH_to_Environment><venv>/CPC_JB/Scripts
	activate.bat

4. Models used (not required for submission)

There are 4 core models required from the Intel Open Model Zoo models which are listed below and can be obtained using the OpenVINO model downloader:

**4.1. Face Detection Model**
```
python3 opt/intel/openvino/deployment_tools/tools/model_downloader/downloader.py --name "face-detection-adas-binary-0001"
```
**4.2. Facial Landmarks Detection Model**
```
python3 opt/intel/openvino/deployment_tools/tools/model_downloader/downloader.py --name "landmarks-regression-retail-0009"
```
**4.3. Head Pose Estimation Model**
```
python3 opt/intel/openvino/deployment_tools/tools/model_downloader/downloader.py --name "head-pose-estimation-adas-0001"
```
**4.4 Gaze Estimation Model**
```
python3 opt/intel/openvino/deployment_tools/tools/model_downloader/downloader.py --name "gaze-estimation-adas-0002"
```

## Demo
The demo of this aplication is easy if you have followed the correct steps above and have all the require files located in the righ folder structure. 
The main.py script controlls the feeding of pre- and post- processed data through the different models initially and they daisey-chain after this.

Below is the demo command line argument to see the demo in action. All model precisions are set to FP32 as if the entire app is being run off the CPU only. These can of course be altered based on virtualised
or real hardware avaliability, where both INT8 or FP16 can be used (if avaliable with each model used by the app). 

 python3 src/main.py -f src/models/intel/face-detection-adas-0001/FP32/face-detection-adas-0001.xml -fl src/models/intel/landmarks-regression-retail-0009/FP32/landmarks-regression-retail-0009.xml -hp src/models/intel/head-pose-estimation-adas-0001/FP32/head-pose-estimation-adas-0001.xml -g src/models/intel/gaze-estimation-adas-0002/FP32/gaze-estimation-adas-0002.xml -i src/bin/demo.mp4 -it video -d CPU -debug headpose gaze

 For ease of execution you need only to run the "party_starter.bat" file from the windows CMD/CLI in the app root (or copy and paste the above).

## Documentation

Command Line Arguments and Requirements 

CLI |Type        |Description
Args|------------| ---------------------------------------------------|
-f  | Mandatory	 | Path to .xml file of Face Detection model.
-l  | Mandatory	 | Path to .xml file of Facial Landmark Detection model.
-hp | Mandatory	 | Path to .xml file of Head Pose Estimation model.
-ge | Mandatory	 | Path to .xml file of Gaze Estimation model.
-i  | Mandatory	 | Path to video file or enter cam for webcam
-it | Mandatory	 | Provide the source of video frames.
-deb| Optional   | To debug each model's output visually, type the model name with comma seperated after --debug
-ld | Optional	 | linker libraries if have any
-d  | Optional	 | Provide the target device: CPU (default) / GPU / MYRIAD / FPGA (MULTI: required for chaining devices & HETERO:FPGA, CPU for FPGA usage)
-h  | Optional   | Help/descriptor for CLI arguments


[Directory-Structure](bin/tree.png)

-<b>bin</b> folder contains the media files

-<b>models</b> folder contains pre-trained models from Open Model Zoo
    - intel
        1. face-detection-adas-binary-0001
        2. gaze-estimation-adas-0002
        3. head-pose-estimation-adas-0001
        4. landmarks-regression-retail-0009
        
-<b>src</b> folder contains python files of the app
    + [constants.py](src/constants.py) : All static constansts of the app located here
    + [face_detection.py](src/face_detection.py) : Face Detection related inference code
    + [facial_landmarks_detection.py](src/facial_landmarks_detection.py) : Landmark Detection related inference code
    + [gaze_estimation.py](src/gaze_estimation.py) : Gaze Estimation related inference code
    + [head_pose_estimation.py](src/head_pose_estimation.py) : Head Pose Estimation related inference code
    + [input_feeder.py](src/input_feeder.py) : input selection related code
    + [model.py](src/model.py) : started code for any pre-trained model
    + [mouse_controller.py](/src/mouse_controller.py) : Mouse Control related utilities.
    + [profiling.py](src/profiling.py) : To check performance of script line by line
    
- <b>README.md</b> File that you are reading right now. 
- <b>.gitignore</b> listing of files that should not be uploaded to GitHub 
- <b>requirements.txt</b> All the dependencies of the project listed here
- <b>runme.bat</b> one shot execution script that covers all the prerequisites of the project.
- <b>start_workbench.bat</b> Helper file from intel to start OpenVino Workbench

## Benchmarks
*TODO:* Include the benchmark results of running your model on multiple hardwares and multiple model precisions. Your benchmarks can include: model loading time, input/output processing time, model inference time etc.


## Benchmarks
Benchmark results of the model.

### FP32

**Inference Time** <br/> 
![inference_time_fp32_image](media/inference_time_fp32.png "Inference Time")

**Frames per Second** <br/> 
![fps_fp32_image](media/fps_fp32.png "Frames per Second")

**Model Loading Time** <br/> 
![model_loading_time_fp32_image](media/model_loading_time_fp32.png "Model Loading Time")

### FP16

**Inference Time** <br/> 
![inference_time_fp16_image](media/inference_time_fp16.png "Inference Time")

**Frames per Second** <br/> 
![fps_fp16_image](media/fps_fp16.png "Frames per Second")

**Model Loading Time** <br/> 
![model_loading_time_fp16_image](media/model_loading_time_fp16.png "Model Loading Time")

### INT8
**Inference Time** <br/> 
![inference_time_int8_image](media/inference_time_int8.png "Inference Time")

**Frames per Second** <br/> 
![fps_int8_image](media/fps_int8.png "Frames per Second")

**Model Loading Time** <br/> 
![model_loading_time_int8_image](media/model_loading_time_int8.png "Model Loading Time")


## Results
*TODO:* Discuss the benchmark results and explain why you are getting the results you are getting. For instance, explain why there is difference in inference time for FP32, FP16 and INT8 models.

## Stand Out Suggestions
This is where you can provide information about the stand out suggestions that you have attempted.

### Async Inference
If you have used Async Inference in your code, benchmark the results and explain its effects on power and performance of your project.

### Edge Cases
There will be certain situations that will break your inference flow. For instance, lighting changes or multiple people in the frame. Explain some of the edge cases you encountered in your project and how you solved them to make your project more robust.
